CPU scheduling is possible because a process is mix of alternate CPU burst and I/O burst.

Scheduler (short term schedular / cpu schedular) selects process from ready Queue to schedule. ready queue may be a fifo queue, priority queue, tree or linked list.

// Scheduling may occur when :
	
	1. process switch from running to waiting state
	2. process switch form running to ready state (due to interrupt)
	3. process switch from waiting state to ready state.
	4. process terminates

	non-preemptive (co-operative) scheduling : once cpu is allocated to process it keep cpu until it releases cpu on its own due to termination or moving to wait state.

	preemptive scheduling : 
		cpu is taken from a process due to interrupt like (end of its time slice)

		it can cause problems when a process modifing data is preempted, it can leave the data in inconsistent state.
		during processing a system call for a process if that process is preempted it can corrupt the kernel data structures. in this case some os waits for sys call to complete before preempting, but this reduces the real time multi-processing.

		some sections of code disable interrupt at entry and enable on exit to prevent from interrupt changing imp code.

// Dispatcher :
	
	its the module that gives control of cpu to process selected by scheduler.
	it does following :
		1. switching context
		2. switching to user mode
		3. jump to proper location.
	time taken by Dispatcher is called dispatch latency.

// Scheduling Criteria :

	1. CPU utilization : it should be maximum
	2. Throughput : no of processes that are completed per unit time.
	3. Turn-around time : time from submission of process till its completion, it includes all, waiting  time, runnin time, i/o etc
	4. waiting time : time spent in ready  queue.
	5. Response time : time from request submission till first response produced, i.e. time it takes for responding.
		the response time should be uniform i.e. variance should be minimum, rather than having faster response in some cases and slower in some cases, its better to have uniform response.

// Scheduling Algorithms :

	// Gantt chart : 

		bar chart that illustrates a particular schedule.


	1. First-Come, First-Serve :

		it can be managed with simple FIFO Queue.
		it is non-preemptive.

		exhibit "Convoy effect" :

			all processes wait for one big process to get off cpu, this results in lower cpu utilization

	2. Shortest-Job-First (shortest next cpu burst) :

			CPU is assigned to process with smallest next cpu burst, if cpu burst are same, then to break the tie, FCFS is used.

			its the optimal algorith i.e. it gives best possible results. because moving a short process before longer one decreases waiting time of shorter more than it increases waiting time for longer process.

			its used frequently in long term scheduling, since user can tell time limit for long term process but it cant be used for short-term scheduling since we cant know length of next cpu burst

			we can approximate the next cpu burst by considering that it'll be similar to previous burst.

				T(n+1) = a* tn + (1-a) * T(n)

				T(n+1) = next cpu burst guess
				tn = actual value of cpu burst for prev 
				T(n) = previous guess of burst

			it can be either preemptive or non-preemptive. 
			preemptive SJF is called shortest-remaining-time-first scheduling.
			if the arrived process has shorter cpu burst than the remaining of running process, then running process is preempted.

	3. Priority Scheduling :

		SJF is speical case of Priority Scheduling where priority is inverse of cpu burst

		highest priority processes are scheduled first.
		it can be  preemptive or non-preemptive.
		preemptive Priority Scheduling algorith will preempt the currrent running process if incoming process has higher priority.

		a problem with this algorith is indefinite blocking or starvation :

			if higher priority process keeps coming then some lower priority process may keep waiting indefinitly

		// ageing : solution to above problem is ageing i.e. gradually increase the priority of processes waiting in ready queue for long period of time.

	4. Round-Robin Scheduling :

		its like FCFS algorithm with preemption. each process is granted cpu for a specific time quantum. after which its preempted and next process is given cpu. it puts the preempted process at the end of queue. new process are also added to the end of queue. it treats ready queue as a circular queue.

		// Processor sharing :
			if time quantum is very large it essentially becomes FCFS algorith. while if the quantum is very small the situation is called Processor sharing. since it create an illusion that there are as many slow processors as the no of processes 

		the time quantum should be much larger than the context switch time otherwise large portion of CPU time will be wasted in context switch.

		rule of thumb is 80% of cpu bursts should be smaller than time quantum

	5. Highest Response Ratio Next (HRRN):

		In this scheduling, processes with highest response ratio is scheduled. This algorithm avoids starvation.

		Response Ratio = (Waiting Time + Burst time) / Burst time

	6. Multi-level Queue Scheduling :

		this class of scheduling is useful if processes can be easily classified into different groups e.g. foreground(interactive), background (batch) etc

		the algorithm divides the ready queue into several queues depending upon the groups of processes like system processes, interactive, batch etc
		each of these queues have thier internal scheduling algorithms, that can be different for different queues. there is again a scheduling algorithm to schedule these queues.
		e.g. the exterior algorithm might be priority based scheduling. and each queue uses FCFS internally, then depending upon the priority process will be selected from one queue

	7. Multi-level Feedback Queue Scheduling :

		in Multi-level Queue Scheduling processes cannot move from one queue to another but this is inflexible.

		in current algorithm we allow a process to move between queues. idea is to seperate processes in terms of cpu bursts. processes with longer bursts are moved to low priority queue and with shorter bursts are moved to high priority queue.

		process waiting too much in low priority queue, is moved to high priority queue due to aging to prevent starvation.

		new process is added to high priority queue, if its burst does not finish within the time quantum then its moved to low priority queue, due to this processes with small time quantum remain in high priority queue and those with longer time quantum goes into low priority queue.

// Thread Scheduling :

	1. Contention Scope :
		threading library will schedule user threads onto the LWP its known as Process Contention Scope (PCS)
		while the kernel schedules kernel level threads onto cpu thats known as System Contention Scope (SCS)
		PCS is done according to thread priority set by programmer.

// Multiple-Processor Scheduling :

	// Asymmetric MultiProcessing : 

		single processor does all scheduling descisions, I/O processing and system activities, while others only run user code.	

	// Symmetric MultiProcessing (SMP):
		
		all processes may be in common ready queue or each processor has its own ready queue. scheduler for each processor selects process to schedule. in such cases it must be programmed well, to ensure no two processor select the same process and processes aren't lost from queue.

	we discuss SMP systems:

	1. Processor Afinity :

		when a process in running on a processor, its data is cached into that processor but if we move that process to some other processor, there's cost of cache clearing and reloading on that processor, so most of SMP systems try to keep process on same processor, this is called Processor Afinity.

		soft-afinity : process is allowed to be moved to other processors
		hard-affinity : process is not allowed to be moved.

		// NUMA : Non uniform memory access
			if processor has some memory attached to it, then accessing that memory is faster for it, and slower for other processors, its called non-uniform memory access.

		for NUMA and processor Afinity to work out better, os must schedule process on processor having Afinity and keep the process in the memory associated with that processor.

	2. Load Balancing :
`
		in SMP system, load balancing tries to keep the load on all the processors even, so that no-one sits idle and no-one is over-burdened.

		its only required when each processor has its separate ready queue. and most of times processors do have separate queue.

		two methods for load balancing:
			1. push migration :
				specific task periodically checks if some processor is overloaded in that case it moves processes to a less busy processesor.
			2. pull migration:
				when a processor is idle it pulls processes from a overloaded processor

		these load balancing techniques sometimes interfare with the processor Afinity, since migration of process removes it from the Afinity processor
		there's a tradeoff here, people prefer to pull migrate when processor is idle and push migrate only if someone is overloaded beyond threshold.
	
	3. Multicore Processors:

		SMP sytems with Multicore are faster and consume less power than systems with each processor on separate chip.